---
title: "01 LightGBM + workflowsets + bayes opt"
author: "Fellipe Gomes"
date: "5/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = F)

library(tidyverse)    # data science toolkit
library(tidymodels)   # machine learning toolkit
library(workflowsets) # optimize workflow (or Pipelines for python users)
library(themis)       # resampling
library(patchwork)    # grid plots

remotes::install_github("curso-r/treesnip", dependencies = FALSE)
library(treesnip)     # lgbm

options(repr.plot.width=12, repr.plot.height=4, warn = -1)

theme_set(theme_bw()) # ggplot theme

set.seed(1234) 

# turn on parallel
doParallel::registerDoParallel(cores = all_cores, parallel::detectCores(logical = FALSE))

local = T
if(local == F){
  path="../input/tabular-playground-series-may-2021//"
}else{
  path=""
} 

# Load data
train <- read_csv(paste0(path, "train.csv"))
test <- read_csv(paste0(path, "test.csv"))
sub <-  read_csv(paste0(path, "sample_submission.csv"))
```

```{r}
# sample 20% for development
train <- train %>% 
  group_by(target) %>% 
  nest() %>% 
  mutate(data = map(data, ~sample_frac(.x, .02))) %>% 
  unnest()
```

Set `recipes`

```{r}
tps_folds <- train %>% rsample::vfold_cv(v = 4,repeats = 2, strata = target)

base_tps_recipe <- recipe(target~., data=train) %>%
  step_rm(id)

rec_filter <- 
  base_tps_recipe %>% 
  step_corr(all_numeric(), threshold = 0.5)

rec_pca <- 
  base_tps_recipe %>% 
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), num_comp = tune())

rec_smote <- 
  base_tps_recipe %>% 
  step_smote(all_outcomes())

rec_under <- 
  base_tps_recipe %>% 
  step_downsample(all_outcomes())
```

Set models

```{r}
glm_spec <- 
  multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

lgb_spec <- boost_tree(
  trees = 10,            # num_iterations
  learn_rate = 0.03,      # eta
  min_n = tune(),         # min_data_in_leaf
  tree_depth = tune(),    # max_depth
  sample_size = tune(),   # bagging_fraction
  mtry = tune(),          # feature_fraction
  loss_reduction = tune() # min_gain_to_split
) %>%  
  set_engine("lightgbm", 
             reg_lambda = tune("reg_lambda"),
             reg_alpha = tune("reg_alpha"),
             num_leaves = tune("num_leaves"),
             #num_leaves = 31,
             early_stop = 10,
             validation = .15) %>% 
  set_mode("classification")
```

Set workflows

```{r}
wflow_params <- parameters(
  min_n(),                  
  tree_depth(c(1, 16)),     
  sample_prop(c(0.1, 0.9)), # to sample_size
  mtry(),                   
  loss_reduction(),         
  reg_lambda = dials::penalty(),
  reg_alpha = dials::penalty(),
  num_leaves = dials::tree_depth(c(15, 255)),
  num_comp())

#wflow_params <- wflow_params %>% update(trees = trees(c(100, 500))) 
wflow_params <- wflow_params %>% finalize(train)
```

Put all together

```{r}
chi_models <- 
  workflow_set(
    preproc = list(pca = rec_pca,
                   filter = rec_filter,
                   smote = rec_smote,
                   under = rec_under),
    models = list(lgb_spec = lgb_spec),
    cross = TRUE
  )
# parameters(chi_models$info[[2]]$workflow[[1]])
# parameters(chi_models)

#chi_models$wflow_id
chi_models <- 
  chi_models %>% 
  anti_join(tibble(wflow_id = c("pca_glmnet", "smote_glmnet", "")), 
            by = "wflow_id")
```

Training models

```{r}
set.seed(123)
chi_models <- 
  chi_models %>% 
  workflow_map("tune_bayes", 
               # Options to grid
               resamples = tps_folds, 
               param_info = wflow_params,
               initial = 15,
               iter = 10, 
               metrics = metric_set(mn_log_loss), 
               control = control_bayes(no_improve = 5, time_limit=20,
                                       verbose = TRUE, save_pred = FALSE),
               # Options to `workflow_map()`
               seed = 3,
               verbose = TRUE)
```

```{r}
g1 <- autoplot(chi_models)+ggtitle("All models")
g2 <- autoplot(chi_models, select_best = TRUE)+ggtitle("Best models")
g1/g2
```

```{r}
(rank_res <- rank_results(chi_models, rank_metric = "mn_log_loss", select_best = TRUE))
```

```{r}
chi_models %>% 
  filter(wflow_id == rank_res$wflow_id[1]) %>% pull(result) %>% .[[1]] %>%
  collect_metrics() %>% 
  select(.metric, .estimator, mean, n, std_err, .config) %>% 
  arrange(mean, -std_err, -n) %>% 
  head(1)
```

```{r}
best_workflow <- 
  chi_models %>% 
  pull_workflow(rank_res$wflow_id[1])

final_parameters <- 
  chi_models %>% 
  filter(wflow_id == rank_res$wflow_id[1]) %>% pull(result) %>% .[[1]] %>%
  select_best("mn_log_loss")

(best_workflow <- best_workflow %>% finalize_workflow(final_parameters))
```

```{r, include=F}
lgbm_tun_fit <- fit(best_workflow, train)
lgbm_tun_pred <- predict(lgbm_tun_fit, test, type = "prob")
```

```{r, eval = F}
lgbm_tun_fit <- fit(best_workflow, train)
lgbm_tun_pred <- predict(lgbm_tun_fit, test, type = "prob")
```

```{r}
sub %>% 
  select(id) %>%
  bind_cols(
    rename_all(lgbm_tun_pred,~stringr::str_remove(.x, "\\.pred_"))) %>% 
  readr::write_csv("lgbm_tun.csv")
```

